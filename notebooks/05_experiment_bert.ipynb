{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cbc93c5-f0d0-4861-b3cf-a1f730aed631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhon\\anaconda3\\envs\\facef\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "C:\\Users\\Jhon\\anaconda3\\envs\\facef\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning de BERT para detección de textos tóxicos\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbc45a6-a2d6-4fe7-ac14-8e11fca741e8",
   "metadata": {},
   "source": [
    "# Fine-tuning de BERT para detección de textos tóxicos\n",
    "\n",
    "A pesar de que el enunciado del problema especifica el uso de modelos basados en árboles de decisión, la exploración de un modelo BERT fine-tuneado representa un valor añadido para el proyecto:\n",
    "\n",
    "- Aunque BERT puede requerir más recursos computacionales, su inclusión en el análisis proporciona un punto de comparación valioso contra los modelos tradicionales.\n",
    "- Puede capturar contextos complejos y matices lingüísticos sutiles, que pueden ser cruciales en la identificación de lenguaje ofensivo indirecto o sarcástico.\n",
    "- Habilidad para manejar palabras desconocidas y variaciones lingüísticas que puede mejorar la robustez ante formas desconocidas de expresión tóxica.\n",
    "- El uso de BERT puede resultar en insights adicionales y potencialmente en una solución más efectiva y adaptable a largo plazo para la detección de toxicidad en textos.\n",
    "\n",
    "Esta exploración adicional nos podria dar una vision mas completa para este problema específico de detección de textos tóxicos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433ddaa5-2596-4de2-9536-ceea6fa355bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de dispositivo (GPU si está disponible, sino CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Cargar los datos\n",
    "df = pd.read_csv('../data/processed/data_toxic_features.csv')\n",
    "\n",
    "# Dividir los datos en entrenamiento y prueba\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    df['message'], df['label'], test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Cargar el tokenizador y el modelo BERT preentrenado en español\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-uncased\"\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bd0eb64-f363-44db-94cb-ce58d5a9a5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jhon\\anaconda3\\envs\\facef\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Tokenizar los textos\n",
    "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "# Crear datasets y dataloaders\n",
    "train_dataset = TensorDataset(\n",
    "    torch.tensor(train_encodings['input_ids']),\n",
    "    torch.tensor(train_encodings['attention_mask']),\n",
    "    torch.tensor(train_labels.tolist())\n",
    ")\n",
    "test_dataset = TensorDataset(\n",
    "    torch.tensor(test_encodings['input_ids']),\n",
    "    torch.tensor(test_encodings['attention_mask']),\n",
    "    torch.tensor(test_labels.tolist())\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Configurar el optimizador\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Función de entrenamiento\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Entrenando\"):\n",
    "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Función de evaluación\n",
    "def evaluate(model, dataloader, device):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    real_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluando\"):\n",
    "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            predictions.extend(preds.cpu().tolist())\n",
    "            real_labels.extend(labels.cpu().tolist())\n",
    "    return classification_report(real_labels, predictions), confusion_matrix(real_labels, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1397a8aa-8abe-451d-ae44-d9895c9c2eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|████████████████████████████████████████████████████████| 24105/24105 [4:30:14<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pérdida de entrenamiento: 0.0942\n",
      "Época 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando: 100%|██████████████████████████████████████████████████████████████| 24105/24105 [4:31:05<00:00,  1.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pérdida de entrenamiento: 0.0822\n",
      "Época 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Entrenando:  97%|████████████████████████████████████████████████████████████  | 23332/24105 [4:22:38<09:01,  1.43it/s]"
     ]
    }
   ],
   "source": [
    "# Entrenar el modelo\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Época {epoch + 1}/{num_epochs}\")\n",
    "    train_loss = train(model, train_loader, optimizer, device)\n",
    "    print(f\"Pérdida de entrenamiento: {train_loss:.4f}\")\n",
    "    \n",
    "    if epoch == num_epochs - 1:  # Evaluar solo en la última época para ahorrar tiempo\n",
    "        print(\"Evaluando el modelo...\")\n",
    "        report, cm = evaluate(model, test_loader, device)\n",
    "        print(\"Informe de clasificación:\")\n",
    "        print(report)\n",
    "\n",
    "# Visualizar la matriz de confusión\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Matriz de Confusión - BERT')\n",
    "plt.ylabel('Etiqueta Verdadera')\n",
    "plt.xlabel('Etiqueta Predicha')\n",
    "plt.show()\n",
    "\n",
    "# Guardar el modelo fine-tuneado\n",
    "model.save_pretrained('../models/bert_finetuned')\n",
    "tokenizer.save_pretrained('../models/bert_finetuned')\n",
    "print(\"Modelo BERT fine-tuneado guardado en '../models/bert_finetuned'\")\n",
    "\n",
    "# Función para predecir la toxicidad de un texto\n",
    "def predict_toxicity(text, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "    prediction = torch.argmax(probabilities, dim=1).item()\n",
    "    toxicity_prob = probabilities[0][1].item()\n",
    "    return prediction, toxicity_prob\n",
    "\n",
    "# Ejemplo de uso\n",
    "example_text = \"Eres un idiota y no sabes nada.\"\n",
    "prediction, toxicity_prob = predict_toxicity(example_text, model, tokenizer, device)\n",
    "print(f\"Texto: '{example_text}'\")\n",
    "print(f\"Predicción: {'Tóxico' if prediction == 1 else 'No tóxico'}\")\n",
    "print(f\"Probabilidad de toxicidad: {toxicity_prob:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92931c8-9b69-4ab0-83ae-db71625790cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
